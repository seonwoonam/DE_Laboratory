{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6803009c-ad67-453f-9e0b-175d4905f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import hour\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5d479-62fb-4d43-9164-d9ef3fdb5f46",
   "metadata": {},
   "source": [
    "## 환경준비\n",
    "- 디멘션 테이블을 팩트 테이블과 join하여 맨허튼 시의 특정 기간동안 count를 세보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744361ab-1889-488a-ba7b-0ff22cb35d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 19:15:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"TLC BroadCast + DPP Demo\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18d5c4f-0ed5-4e14-b62a-e2fb8cfc4b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 팩트 데이터 로딩\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"VendorID\",               IntegerType(),  True),\n",
    "#     StructField(\"tpep_pickup_datetime\",   TimestampType(),True),\n",
    "#     StructField(\"tpep_dropoff_datetime\",  TimestampType(),True),\n",
    "#     StructField(\"passenger_count\",        IntegerType(),  True),\n",
    "#     StructField(\"trip_distance\",          DoubleType(),   True),\n",
    "#     StructField(\"RatecodeID\",             IntegerType(),  True),\n",
    "#     StructField(\"store_and_fwd_flag\",     StringType(),   True),\n",
    "#     StructField(\"PULocationID\",           LongType(),     True),   # ← INT32·INT64 섞임 대비\n",
    "#     StructField(\"DOLocationID\",           LongType(),     True),\n",
    "#     StructField(\"payment_type\",           IntegerType(),  True),\n",
    "#     StructField(\"fare_amount\",            DoubleType(),   True),\n",
    "#     StructField(\"extra\",                  DoubleType(),   True),\n",
    "#     StructField(\"mta_tax\",                DoubleType(),   True),\n",
    "#     StructField(\"tip_amount\",             DoubleType(),   True),\n",
    "#     StructField(\"tolls_amount\",           DoubleType(),   True),\n",
    "#     StructField(\"improvement_surcharge\",  DoubleType(),   True),\n",
    "#     StructField(\"total_amount\",           DoubleType(),   True),\n",
    "#     StructField(\"congestion_surcharge\",   DoubleType(),   True),\n",
    "#     StructField(\"airport_fee\",   DoubleType(),   True),\n",
    "# ])\n",
    "trips = spark.read.parquet(\"data/yellow_tripdata_2024-*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c711b09a-6b17-4e78-b8d6-f1101c50c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디멘션 데이터 로딩\n",
    "zone_path = \"zone_data/taxi_zone_lookup.csv\"\n",
    "zones  = (\n",
    "    spark.read.option(\"header\", True).csv(zone_path)\n",
    "         .selectExpr(\"LocationID\", \"Zone\", \"Borough\")\n",
    "         .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f8837f-6dfe-4830-a391-c1d22506ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "37501349\n"
     ]
    }
   ],
   "source": [
    "print(trips.rdd.getNumPartitions())\n",
    "print(trips.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5fa1fd-ed6e-47a3-80e4-6586e6e9591b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'Airport_fee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79441257-a4fb-4080-810e-a2916b563819",
   "metadata": {},
   "source": [
    "## 최적화 OFF 버전\n",
    "- Broadcast & DPP 비활성화\n",
    "- 실험조건\n",
    "  - 2024 택시 데이터\n",
    "  - 디멘션 테이블과 Join 후에 맨허턴 Borugh에서의 택시 픽업 Zone 수 찾기\n",
    "- 실험결과\n",
    "  - all off : 5.04초(sort merge join)\n",
    "  - AQE on / Broadcast join off / DPP off : 6.38초 (sort merge join 일어남)\n",
    "  - AQE off / Broadcast join on / DPP on : 1.8초 (BroadcastHashJoin)\n",
    "  - all on : 1.58초 (BroadcastHashJoin)\n",
    "- 결론\n",
    "  - DPP나 Broadcast join 기능을 꺼버리고 AQE만 실행해놓으면 조인전략개선 같은 기능을 사용할 수 없다. 그래서 위에 상황에서는 AQE 오버헤드만 늘어서 시간이 오히려 증가\n",
    "  - AQE를 아무 상황에나 사용한다해서 나아지지 않는다. 파티션이 어느정도 많아야 적절한 파티션 병합기능이 활성화될 것이고, skew 혹은 join과정도 없다면 overhead만 늘어날 수 있다.\n",
    "  - 데이터 쿼리가 아주 작고, 파티션 적으며, 플랜을 개발자가 알아야 할 때는 AQE 꺼도된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efec659c-a7fa-4b7b-af21-338c4f03e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join, DPP 꺼버리기 \n",
    "# 예상 풀스캔 후 Shuffle Hash join 발생할듯 하다.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"false\")\n",
    "\n",
    "# AQE도 꺼야 join 전략을 중간에 수정안할듯.\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e45a920-2582-48fb-81c5-35cfa8116934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN TaskMemoryManager: Failed to allocate a page (134217728 bytes), try again.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.548s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 10.0 (TID 27): Retried waiting for GCLocker too often allocating 16777218 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/20 19:15:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO OPT] Rows = 67, elapsed = 6.63s\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (20)\n",
      "+- Sort (19)\n",
      "   +- Exchange (18)\n",
      "      +- HashAggregate (17)\n",
      "         +- Exchange (16)\n",
      "            +- HashAggregate (15)\n",
      "               +- Project (14)\n",
      "                  +- SortMergeJoin Inner (13)\n",
      "                     :- Sort (4)\n",
      "                     :  +- Exchange (3)\n",
      "                     :     +- Filter (2)\n",
      "                     :        +- Scan parquet  (1)\n",
      "                     +- Sort (12)\n",
      "                        +- Exchange (11)\n",
      "                           +- Project (10)\n",
      "                              +- Filter (9)\n",
      "                                 +- InMemoryTableScan (5)\n",
      "                                       +- InMemoryRelation (6)\n",
      "                                             +- * Project (8)\n",
      "                                                +- Scan csv  (7)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [PULocationID#7]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials/missions/W4/M2/data/yellow_tripdata_2024-01.parquet, ... 10 entries]\n",
      "PushedFilters: [IsNotNull(PULocationID)]\n",
      "ReadSchema: struct<PULocationID:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [PULocationID#7]\n",
      "Condition : isnotnull(PULocationID#7)\n",
      "\n",
      "(3) Exchange\n",
      "Input [1]: [PULocationID#7]\n",
      "Arguments: hashpartitioning(PULocationID#7, 200), ENSURE_REQUIREMENTS, [plan_id=468]\n",
      "\n",
      "(4) Sort\n",
      "Input [1]: [PULocationID#7]\n",
      "Arguments: [PULocationID#7 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) InMemoryTableScan\n",
      "Output [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "Arguments: [Borough#56, LocationID#55, Zone#57], [isnotnull(Borough#56), (Borough#56 = Manhattan), isnotnull(LocationID#55)]\n",
      "\n",
      "(6) InMemoryRelation\n",
      "Arguments: [LocationID#55, Zone#57, Borough#56], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@413fcf37,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [LocationID#55, Zone#57, Borough#56]\n",
      "+- FileScan csv [LocationID#55,Borough#56,Zone#57] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LocationID:string,Borough:string,Zone:string>\n",
      ",None)\n",
      "\n",
      "(7) Scan csv \n",
      "Output [3]: [LocationID#55, Borough#56, Zone#57]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials/missions/W4/M2/zone_data/taxi_zone_lookup.csv]\n",
      "ReadSchema: struct<LocationID:string,Borough:string,Zone:string>\n",
      "\n",
      "(8) Project [codegen id : 1]\n",
      "Output [3]: [LocationID#55, Zone#57, Borough#56]\n",
      "Input [3]: [LocationID#55, Borough#56, Zone#57]\n",
      "\n",
      "(9) Filter\n",
      "Input [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "Condition : ((isnotnull(Borough#56) AND (Borough#56 = Manhattan)) AND isnotnull(LocationID#55))\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [LocationID#55, Zone#57]\n",
      "Input [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "\n",
      "(11) Exchange\n",
      "Input [2]: [LocationID#55, Zone#57]\n",
      "Arguments: hashpartitioning(cast(LocationID#55 as int), 200), ENSURE_REQUIREMENTS, [plan_id=469]\n",
      "\n",
      "(12) Sort\n",
      "Input [2]: [LocationID#55, Zone#57]\n",
      "Arguments: [cast(LocationID#55 as int) ASC NULLS FIRST], false, 0\n",
      "\n",
      "(13) SortMergeJoin\n",
      "Left keys [1]: [PULocationID#7]\n",
      "Right keys [1]: [cast(LocationID#55 as int)]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [1]: [Zone#57]\n",
      "Input [3]: [PULocationID#7, LocationID#55, Zone#57]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [1]: [Zone#57]\n",
      "Keys [1]: [Zone#57]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#307L]\n",
      "Results [2]: [Zone#57, count#308L]\n",
      "\n",
      "(16) Exchange\n",
      "Input [2]: [Zone#57, count#308L]\n",
      "Arguments: hashpartitioning(Zone#57, 200), ENSURE_REQUIREMENTS, [plan_id=476]\n",
      "\n",
      "(17) HashAggregate\n",
      "Input [2]: [Zone#57, count#308L]\n",
      "Keys [1]: [Zone#57]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#173L]\n",
      "Results [2]: [Zone#57, count(1)#173L AS count#174L]\n",
      "\n",
      "(18) Exchange\n",
      "Input [2]: [Zone#57, count#174L]\n",
      "Arguments: rangepartitioning(count#174L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=479]\n",
      "\n",
      "(19) Sort\n",
      "Input [2]: [Zone#57, count#174L]\n",
      "Arguments: [count#174L DESC NULLS LAST], true, 0\n",
      "\n",
      "(20) AdaptiveSparkPlan\n",
      "Output [2]: [Zone#57, count#174L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 특정기간에서 맨허튼 픽업 건수 구하기\n",
    "start = time.time()\n",
    "result_no_option = (\n",
    "    trips\n",
    "        # .filter(\"tpep_pickup_datetime BETWEEN '2024-06-15' AND '2024-12-31'\")\n",
    "        .join(zones, trips.PULocationID == zones.LocationID, \"inner\")\n",
    "        .filter(F.col(\"Borough\") == \"Manhattan\")\n",
    "        .groupBy(\"Zone\")\n",
    "        .count()\n",
    "        .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "print(f\"[NO OPT] Rows = {result_no_option.count()}, elapsed = {time.time()-start:.2f}s\")\n",
    "# explain 함수는 Catalyst optimizer가 하는 실행 계획을 보여줌\n",
    "result_no_option.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7ec4a9-882a-4eff-ae1b-27e7b1fce0ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'queryExecution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_no_option\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryExecution\u001b[49m\u001b[38;5;241m.\u001b[39mexecutedPlan\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'queryExecution'"
     ]
    }
   ],
   "source": [
    "result_no_option.explain(mode=\"extended\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f42afc-fa33-4ad3-881e-4ce4608dba17",
   "metadata": {},
   "source": [
    "## 최적화 ON 버전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa94b014-cb9f-4cf2-b5e5-f34176f33583",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100*1024*1024)\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4b9cd4-136c-4213-8927-3ea920000d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WITH OPT] Rows = 67, elapsed = 0.84s\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (17)\n",
      "+- Sort (16)\n",
      "   +- Exchange (15)\n",
      "      +- HashAggregate (14)\n",
      "         +- Exchange (13)\n",
      "            +- HashAggregate (12)\n",
      "               +- Project (11)\n",
      "                  +- BroadcastHashJoin Inner BuildRight (10)\n",
      "                     :- Filter (2)\n",
      "                     :  +- Scan parquet  (1)\n",
      "                     +- BroadcastExchange (9)\n",
      "                        +- Project (8)\n",
      "                           +- Filter (7)\n",
      "                              +- InMemoryTableScan (3)\n",
      "                                    +- InMemoryRelation (4)\n",
      "                                          +- * Project (6)\n",
      "                                             +- Scan csv  (5)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [PULocationID#7]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials/missions/W4/M2/data/yellow_tripdata_2024-01.parquet, ... 10 entries]\n",
      "PushedFilters: [IsNotNull(PULocationID)]\n",
      "ReadSchema: struct<PULocationID:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [PULocationID#7]\n",
      "Condition : isnotnull(PULocationID#7)\n",
      "\n",
      "(3) InMemoryTableScan\n",
      "Output [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "Arguments: [Borough#56, LocationID#55, Zone#57], [isnotnull(Borough#56), (Borough#56 = Manhattan), isnotnull(LocationID#55)]\n",
      "\n",
      "(4) InMemoryRelation\n",
      "Arguments: [LocationID#55, Zone#57, Borough#56], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@30e9de68,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [LocationID#55, Zone#57, Borough#56]\n",
      "+- FileScan csv [LocationID#55,Borough#56,Zone#57] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LocationID:string,Borough:string,Zone:string>\n",
      ",None)\n",
      "\n",
      "(5) Scan csv \n",
      "Output [3]: [LocationID#55, Borough#56, Zone#57]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/seonwoo/Documents/GitHub/data_engineering_course_materials/missions/W4/M2/zone_data/taxi_zone_lookup.csv]\n",
      "ReadSchema: struct<LocationID:string,Borough:string,Zone:string>\n",
      "\n",
      "(6) Project [codegen id : 1]\n",
      "Output [3]: [LocationID#55, Zone#57, Borough#56]\n",
      "Input [3]: [LocationID#55, Borough#56, Zone#57]\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "Condition : ((isnotnull(Borough#56) AND (Borough#56 = Manhattan)) AND isnotnull(LocationID#55))\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [LocationID#55, Zone#57]\n",
      "Input [3]: [Borough#56, LocationID#55, Zone#57]\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [2]: [LocationID#55, Zone#57]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(cast(input[0, string, true] as int) as bigint)),false), [plan_id=725]\n",
      "\n",
      "(10) BroadcastHashJoin\n",
      "Left keys [1]: [PULocationID#7]\n",
      "Right keys [1]: [cast(LocationID#55 as int)]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [1]: [Zone#57]\n",
      "Input [3]: [PULocationID#7, LocationID#55, Zone#57]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [1]: [Zone#57]\n",
      "Keys [1]: [Zone#57]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#506L]\n",
      "Results [2]: [Zone#57, count#507L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [2]: [Zone#57, count#507L]\n",
      "Arguments: hashpartitioning(Zone#57, 200), ENSURE_REQUIREMENTS, [plan_id=730]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [2]: [Zone#57, count#507L]\n",
      "Keys [1]: [Zone#57]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#375L]\n",
      "Results [2]: [Zone#57, count(1)#375L AS count#376L]\n",
      "\n",
      "(15) Exchange\n",
      "Input [2]: [Zone#57, count#376L]\n",
      "Arguments: rangepartitioning(count#376L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=733]\n",
      "\n",
      "(16) Sort\n",
      "Input [2]: [Zone#57, count#376L]\n",
      "Arguments: [count#376L DESC NULLS LAST], true, 0\n",
      "\n",
      "(17) AdaptiveSparkPlan\n",
      "Output [2]: [Zone#57, count#376L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정기간에서 맨허튼 픽업 건수 구하기\n",
    "start = time.time()\n",
    "result_with_option = (\n",
    "       trips\n",
    "        # .filter(\"tpep_pickup_datetime BETWEEN '2024-06-15' AND '2024-12-31'\")\n",
    "        .join(zones, trips.PULocationID == zones.LocationID, \"inner\")\n",
    "        .filter(F.col(\"Borough\") == \"Manhattan\")\n",
    "        .groupBy(\"Zone\")\n",
    "        .count()\n",
    "        .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "print(f\"[WITH OPT] Rows = {result_with_option.count()}, elapsed = {time.time()-start:.2f}s\")\n",
    "# explain 함수는 Catalyst optimizer가 하는 실행 계획을 보여줌\n",
    "result_with_option.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac6841-4b3c-48ff-8772-27f19df8bac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
